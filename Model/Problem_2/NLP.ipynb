{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "h0sdlAv6YL11"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import string\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, precision_score, recall_score, accuracy_score, balanced_accuracy_score, ConfusionMatrixDisplay\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuaB8r1_YgWU",
        "outputId": "8a7dfa1b-b84e-45f8-c730-7d2039808bb1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
            "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
            "[nltk_data]     unable to get local issuer certificate (_ssl.c:1125)>\n",
            "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
            "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
            "[nltk_data]     unable to get local issuer certificate (_ssl.c:1125)>\n",
            "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
            "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
            "[nltk_data]     unable to get local issuer certificate (_ssl.c:1125)>\n",
            "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
            "[nltk_data]     [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify\n",
            "[nltk_data]     failed: unable to get local issuer certificate\n",
            "[nltk_data]     (_ssl.c:1125)>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/lequochuy/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/lequochuy/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/lequochuy/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/lequochuy/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import ssl\n",
        "import nltk\n",
        "\n",
        "# Bypass SSL verification temporarily\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# Download the necessary data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "import ssl\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "#from nltk.tokenize import word_tokenize, pos_tag\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-lcJsNOCYi-o"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.feature_selection import SelectKBest, chi2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iyDRmdn4YmRR"
      },
      "outputs": [],
      "source": [
        "path_prepost = \"/Users/lequochuy/Documents/Research/UIT/bài toán 2\"\n",
        "# path_data = os.path.join(root,\"PROJECTS/EDUCATION/DATA\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sRmv4VhAYucJ"
      },
      "outputs": [],
      "source": [
        "def ReadExcel(original,dirt):\n",
        "  answer_path = os.path.join(original,dirt)\n",
        "  answer = pd.read_excel(answer_path)\n",
        "  return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CJfHNYKlY1hG"
      },
      "outputs": [],
      "source": [
        "def ReadCSV(original,dirt):\n",
        "  answer_path = os.path.join(original,dirt)\n",
        "  answer = pd.read_csv(answer_path)\n",
        "  return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LAJLVGPeY3TJ"
      },
      "outputs": [],
      "source": [
        "def HistogramData(df_data,namefields):\n",
        "  for name in namefields:\n",
        "    n, bins = np.histogram(df_data[name])\n",
        "    plt.hist(df_data[name],bins)\n",
        "    print(bins)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hT1Kot2HY6qa"
      },
      "outputs": [],
      "source": [
        "def AddCountField(df_data):\n",
        "  df_res = df_data.copy()\n",
        "  for name, group in df_res.groupby('Mã số ID'):\n",
        "    #print(f\"Group: {name,len(group)}\")\n",
        "    # Process elements within the group (e.g., print, calculate statistics)\n",
        "    df_res.loc[df_res['Mã số ID'] == name,'count'] = range(1, (df_res['Mã số ID'] == name).sum()+1)#/(df_res['Mã số ID'] == name).sum()\n",
        "  return df_res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ezuUJ_aqY83O"
      },
      "outputs": [],
      "source": [
        "def AssignLabel(df_data):\n",
        "  df_data.loc[(df_data['Q. 1 /10.00'] >= 9) & (df_data['Q. 1 /10.00'] <=10),'Q. 1 /10.00'] = 10\n",
        "  df_data.loc[(df_data['Q. 1 /10.00'] >= 8) & (df_data['Q. 1 /10.00'] < 9),'Q. 1 /10.00'] = 9\n",
        "  df_data.loc[(df_data['Q. 1 /10.00'] >= 7) & (df_data['Q. 1 /10.00'] < 8),'Q. 1 /10.00'] = 8\n",
        "  df_data.loc[(df_data['Q. 1 /10.00'] >= 6) & (df_data['Q. 1 /10.00'] < 7),'Q. 1 /10.00'] = 7\n",
        "  df_data.loc[(df_data['Q. 1 /10.00'] >= 5) & (df_data['Q. 1 /10.00'] < 6),'Q. 1 /10.00'] = 6\n",
        "  df_data.loc[(df_data['Q. 1 /10.00'] >= 4) & (df_data['Q. 1 /10.00'] < 5),'Q. 1 /10.00'] = 5\n",
        "  df_data.loc[(df_data['Q. 1 /10.00'] >= 3) & (df_data['Q. 1 /10.00'] < 4),'Q. 1 /10.00'] = 4\n",
        "  df_data.loc[(df_data['Q. 1 /10.00'] >= 2) & (df_data['Q. 1 /10.00'] < 3),'Q. 1 /10.00'] = 3\n",
        "  df_data.loc[(df_data['Q. 1 /10.00'] >= 1) & (df_data['Q. 1 /10.00'] < 2),'Q. 1 /10.00'] = 2\n",
        "  df_data.loc[df_data['Q. 1 /10.00'] < 1,'Q. 1 /10.00'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "T1G37xmSY_zs"
      },
      "outputs": [],
      "source": [
        "def ReadLabFiles(original,df_files):\n",
        "  df_labs = pd.DataFrame()\n",
        "  for idx, row in df_files.iterrows():\n",
        "    ans = ReadCSV(original,row[\"content\"])\n",
        "    ans = AddCountField(ans)\n",
        "\n",
        "    print(ans.shape)\n",
        "\n",
        "    df_labs = pd.concat([df_labs, ans], ignore_index=True)\n",
        "    print(df_labs.shape)\n",
        "\n",
        "    #print(ans.loc[0:5,['Mã số ID','Điểm/10.00']])\n",
        "  return df_labs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "z5pcm5a1ZBjy"
      },
      "outputs": [],
      "source": [
        "def GetStudentInfo(df_data, IDST):\n",
        "  return df_data[df_data['Mã số ID'] == IDST]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LjyyjLf8ZELr"
      },
      "outputs": [],
      "source": [
        "def right_substring_to_char(s, leftchar, rightchar):\n",
        "  \"\"\"Returns the right substring of s starting from the last occurrence of char.\n",
        "\n",
        "  Args:\n",
        "    s: The string to extract the substring from.\n",
        "    char: The character to find the starting index from.\n",
        "\n",
        "  Returns:\n",
        "    The right substring of s starting from the last occurrence of char,\n",
        "    or the entire string if char is not found.\n",
        "  \"\"\"\n",
        "  last_index = s.rfind('/')\n",
        "  dir = s[:last_index+1]\n",
        "  left_index = s.rfind(leftchar)\n",
        "  if left_index != -1:\n",
        "    sleft = s[left_index + 1:]\n",
        "    right_index = sleft.rfind(rightchar)\n",
        "    return dir + sleft[:right_index] + '.txt'\n",
        "  else:\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UFx58fBdZHtR"
      },
      "outputs": [],
      "source": [
        "def ReadTextFromExcel(original,dirt):\n",
        "  token_dict = {}\n",
        "  text = \"\"\n",
        "  fexcel = ReadExcel(original,dirt)\n",
        "  for idx, row in fexcel.iterrows():\n",
        "    path_row = row['content']\n",
        "    ftext = right_substring_to_char(path_row,'[',']')\n",
        "    print(ftext)\n",
        "    with open (os.path.join(original,ftext)) as content:\n",
        "      text = content.read()\n",
        "      text = text_clean(text)\n",
        "      token_dict[ftext] = text\n",
        "  return token_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kZIgdkE6byQh"
      },
      "outputs": [],
      "source": [
        "def text_clean(text):\n",
        "    text = re.sub(r\"\\n\",\" \",text)   #remove line breaks\n",
        "    text = text.lower() #convert to lowercase\n",
        "    text = re.sub(r\"\\d+\",\"\",text)   #remove digits and currencies\n",
        "    text = re.sub(r'[\\$\\d+\\d+\\$]', \"\", text)\n",
        "    text = re.sub(r'[()<>::]', \" \", text)\n",
        "    text = re.sub(r'\\d+[\\.\\/-]\\d+[\\.\\/-]\\d+', '', text)   #remove dates\n",
        "    text = re.sub(r'\\d+[\\.\\/-]\\d+[\\.\\/-]\\d+', '', text)\n",
        "    text = re.sub(r'\\d+[\\.\\/-]\\d+[\\.\\/-]\\d+', '', text)\n",
        "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text)   #remove non-ascii\n",
        "    text = re.sub(r'[^\\w\\s]','',text)   #remove punctuation\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', ' ', text)   #remove hyperlinks\n",
        "    text = re.sub(r'\\b\\w{1,3}\\b', '',text) # remove words have length from 1 to 3\n",
        "    text = re.sub(r'\\b\\w{11,20}\\b', '',text) # remove words have length from 1 to 3\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YNEAHztOnSHE"
      },
      "outputs": [],
      "source": [
        "def text_sementic(text, method, rm_stop):\n",
        "    text = text_clean(text)\n",
        "    if rm_stop == True:\n",
        "        filtered_tokens = [word for word in word_tokenize(text) if not word in set(stopwords.words('english'))]\n",
        "        text = \" \".join(filtered_tokens)\n",
        "\n",
        "    if method == 'L':\n",
        "        lemmer = WordNetLemmatizer()\n",
        "        lemm_tokens = [lemmer.lemmatize(word) for word in word_tokenize(text)]\n",
        "        return \" \".join(lemm_tokens)\n",
        "\n",
        "    #stemming\n",
        "    if method == 'S':\n",
        "        porter = PorterStemmer()\n",
        "        stem_tokens = [porter.stem(word) for word in word_tokenize(text)]\n",
        "        return \" \".join(stem_tokens)\n",
        "    if method == 'T':\n",
        "      text = lemmatize_passage(text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Z7Af1YpTJRyO"
      },
      "outputs": [],
      "source": [
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "def lemmatize_passage(text):\n",
        "    words = word_tokenize(text)\n",
        "    pos_tags = pos_tag(words)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
        "    lemmatized_sentence = ' '.join(lemmatized_words)\n",
        "    return lemmatized_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "BEjhEx-e8Kv_"
      },
      "outputs": [],
      "source": [
        "def Words2Text(words):\n",
        "  text = \" \".join(words)\n",
        "  return text;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "iyvMnonVEBab"
      },
      "outputs": [],
      "source": [
        "def Text2Words(text):\n",
        "  words = [word for word in word_tokenize(text)]\n",
        "  return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "9-RQdmZab1ZY"
      },
      "outputs": [],
      "source": [
        "def data_w_tfidf_vectorizer(preprocessed_text, vocab = None):\n",
        "    #vectorize dataset\n",
        "    tfidf = TfidfVectorizer(stop_words='english',use_idf=True,vocabulary = vocab)\n",
        "    vectorized_data = tfidf.fit_transform(preprocessed_text)\n",
        "    return tfidf, vectorized_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Mk8xpzZmb5QF"
      },
      "outputs": [],
      "source": [
        "def InsertFeature(df_data,featurename, feature):\n",
        "  df_res = df_data[['Mã số ID','count']].copy()\n",
        "  df_res.loc[:,featurename] = feature.copy()\n",
        "\n",
        "  df_res['effort'] = df_res['count'] / df_res.groupby('Mã số ID')['count'].transform('count')\n",
        "  #df_res = pd.concat([df_res, df_data[['Mã số ID','count']].copy()])\n",
        "  #df_res = pd.concat([df_res, feature.copy()])\n",
        "  return df_res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "q1rOPyALb9KH"
      },
      "outputs": [],
      "source": [
        "def AddFeatureToAnswer(tfidf, original,dirt, dict_text):\n",
        "  text = \"\"\n",
        "\n",
        "  #df_reslabel = pd.DataFrame(columns=['grad'])\n",
        "  feature_names = tfidf.get_feature_names_out()\n",
        "\n",
        "  df_tkappend = pd.DataFrame()\n",
        "\n",
        "  df_labelappend = pd.DataFrame(columns=['grad'])\n",
        "  df_reslabel = pd.DataFrame(columns=['grad'])\n",
        "\n",
        "  fexcel = ReadExcel(original,dirt)\n",
        "  for idx, row in fexcel.iterrows():\n",
        "    df_reslabel = pd.DataFrame(columns=['grad'])\n",
        "    df_res = pd.DataFrame()\n",
        "    df_ans = pd.DataFrame()\n",
        "\n",
        "    path_row = row['content']\n",
        "    print(path_row)\n",
        "    df_ans = ReadCSV(original,row[\"content\"])\n",
        "    df_ans = AddCountField(df_ans)\n",
        "    AssignLabel(df_ans)\n",
        "    ftext = right_substring_to_char(path_row,'[',']')\n",
        "    with open (os.path.join(original,ftext)) as content:\n",
        "      text = content.read()\n",
        "      #print(text)\n",
        "      text = text_sementic(text, 'T', False)\n",
        "      #print(text)\n",
        "    #update dictionary\n",
        "    if ftext not in dict_text.keys():\n",
        "      dict_text[ftext] = text\n",
        "    response = tfidf.transform([text])\n",
        "    feature = response.toarray()[0]\n",
        "\n",
        "    df_res = InsertFeature(df_ans,feature_names,feature)\n",
        "    print(df_res.shape,df_reslabel.shape)\n",
        "    print(df_res.head())\n",
        "    #print(df_res.tail)\n",
        "    df_reslabel['grad'] = df_ans['Q. 1 /10.00'].copy()\n",
        "    df_tkappend = pd.concat([df_tkappend, df_res], ignore_index=True)\n",
        "    df_labelappend = pd.concat([df_labelappend, df_reslabel], ignore_index=True)\n",
        "\n",
        "  df_tkappend['Mã số ID'] = pd.to_numeric(df_tkappend['Mã số ID'])\n",
        "  df_labelappend['grad'] = pd.to_numeric(df_labelappend['grad'])\n",
        "  return df_tkappend, df_labelappend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "89LnJZ0vT973"
      },
      "outputs": [],
      "source": [
        "def WeightedEffort(df):\n",
        "  df['effort'] = df['count'] / df.groupby('Mã số ID')['count'].transform('count')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "A7zlvTZ9cA4O"
      },
      "outputs": [],
      "source": [
        "def ReadPreLabs(original,filepath):\n",
        "  Labs_info = ReadExcel(original,filepath)\n",
        "  df_prelabs = ReadLabFiles(original,Labs_info)  # add count\n",
        "  AssignLabel(df_prelabs)                        # add label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "3lzNA137cFEh"
      },
      "outputs": [],
      "source": [
        "def ModelData(vectorized_data, label_train, name_model, alpha = 0.1):\n",
        "    print(vectorized_data.shape, label_train.shape)\n",
        "    if name_model == 'Naive Bayes':\n",
        "      print(name_model)\n",
        "      #define model\n",
        "      model = MultinomialNB(alpha=0.1)\n",
        "      model.fit(vectorized_data, label_train)\n",
        "    elif name_model == 'KNN':\n",
        "      print(name_model)\n",
        "    ## K-Nearest Neighbors\n",
        "      model = KNeighborsClassifier(n_neighbors=3, p =1) #24\n",
        "      model.fit(vectorized_data, label_train)\n",
        "    ## name_model\n",
        "    elif name_model == 'Random Forests':\n",
        "      print(name_model)\n",
        "      #model = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "      model = RandomForestClassifier(max_depth=3, min_samples_leaf=10, n_estimators=100, random_state=42)\n",
        "      model.fit(vectorized_data, label_train)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "pFSbz-dxcH6R"
      },
      "outputs": [],
      "source": [
        "def EvaluateModel(model, data_test, label_test):\n",
        "    #evaluate model\n",
        "    print(data_test.shape,label_test.shape)\n",
        "    predictions = model.predict(data_test)\n",
        "\n",
        "    #predictions = predictions.astype(int)\n",
        "    #label_test = label_test.astype(int)\n",
        "\n",
        "    # pred = pd.DataFrame(predictions.astype(int),columns=['Pred'])\n",
        "    # pred.to_excel(os.path.join(path_prepost,\"POST_LAB1/\"+\"OutputPrediction.xlsx\"))\n",
        "    # label_test.to_excel(os.path.join(path_prepost,\"POST_LAB1/\"+\"OutputPredictionLabel.xlsx\"))\n",
        "    # print(predictions[0:5])\n",
        "\n",
        "    accuracy = accuracy_score( label_test['grad'], predictions)\n",
        "    balanced_accuracy = balanced_accuracy_score(label_test, predictions)\n",
        "    precision = precision_score(label_test, predictions,average='weighted')\n",
        "\n",
        "    print(\"Accuracy:\",round(100*accuracy,2),'%')\n",
        "    print(\"Balanced accuracy:\",round(100*balanced_accuracy,2),'%')\n",
        "    print(\"Precision:\", round(100*precision,2),'%')\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "4NgJ8W97gtqY"
      },
      "outputs": [],
      "source": [
        "def BestKNN(df_train, df_label):\n",
        "  k_values = [i for i in range (1,31)]\n",
        "  scores = []\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  X = scaler.fit_transform(df_train)\n",
        "\n",
        "  for k in k_values:\n",
        "      knn = KNeighborsClassifier(n_neighbors=k)\n",
        "      score = cross_val_score(knn, X, df_label, cv=5)\n",
        "      scores.append(np.mean(score))\n",
        "  sns.lineplot(x = k_values, y = scores, marker = 'o')\n",
        "  plt.xlabel(\"K Values\")\n",
        "  plt.ylabel(\"Accuracy Score\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJKpf26UcWMp"
      },
      "source": [
        "## Load data transform to vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UsssbRqcK8O",
        "outputId": "fbb5c13d-7185-4ad4-c7a5-857048251d20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PRE_LAB1/Q 1.1.txt\n",
            "PRE_LAB1/Q 1.2.txt\n",
            "PRE_LAB1/Q 1.5.txt\n",
            "PRE_LAB1/Q 2.7.txt\n",
            "PRE_LAB1/Q 2.6.txt\n",
            "PRE_LAB1/Q 3.1.txt\n",
            "PRE_LAB1/Q 3.2.txt\n",
            "PRE_LAB1/Q 3.3.txt\n",
            "PRE_LAB1/Q 3.7.txt\n",
            "PRE_LAB1/Q 4.2.txt\n",
            "10\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "dict_keys(['PRE_LAB1/Q 1.1.txt', 'PRE_LAB1/Q 1.2.txt', 'PRE_LAB1/Q 1.5.txt', 'PRE_LAB1/Q 2.7.txt', 'PRE_LAB1/Q 2.6.txt', 'PRE_LAB1/Q 3.1.txt', 'PRE_LAB1/Q 3.2.txt', 'PRE_LAB1/Q 3.3.txt', 'PRE_LAB1/Q 3.7.txt', 'PRE_LAB1/Q 4.2.txt'])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dic_text = ReadTextFromExcel(path_prepost,\"PRE_LAB1/PreLab01.xlsx\")\n",
        "print(len(dic_text))\n",
        "dic_text.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/lequochuy/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "XTuJmESssGn8"
      },
      "outputs": [
        {
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/Users/lequochuy/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.8/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.8/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.8/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/lequochuy/nltk_data'\n**********************************************************************\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32m/Users/lequochuy/Documents/Research/UIT/bài toán 2/NLP.ipynb Cell 31\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lequochuy/Documents/Research/UIT/b%C3%A0i%20to%C3%A1n%202/NLP.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#preprocessed_text = [text_sementic(text, 'S', False) for text in dic_text.values()]\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lequochuy/Documents/Research/UIT/b%C3%A0i%20to%C3%A1n%202/NLP.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m preprocessed_text \u001b[39m=\u001b[39m [text_sementic(text, \u001b[39m'\u001b[39m\u001b[39mT\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m dic_text\u001b[39m.\u001b[39mvalues()]\n",
            "\u001b[1;32m/Users/lequochuy/Documents/Research/UIT/bài toán 2/NLP.ipynb Cell 31\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lequochuy/Documents/Research/UIT/b%C3%A0i%20to%C3%A1n%202/NLP.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#preprocessed_text = [text_sementic(text, 'S', False) for text in dic_text.values()]\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lequochuy/Documents/Research/UIT/b%C3%A0i%20to%C3%A1n%202/NLP.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m preprocessed_text \u001b[39m=\u001b[39m [text_sementic(text, \u001b[39m'\u001b[39;49m\u001b[39mT\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m dic_text\u001b[39m.\u001b[39mvalues()]\n",
            "\u001b[1;32m/Users/lequochuy/Documents/Research/UIT/bài toán 2/NLP.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lequochuy/Documents/Research/UIT/b%C3%A0i%20to%C3%A1n%202/NLP.ipynb#X41sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(stem_tokens)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lequochuy/Documents/Research/UIT/b%C3%A0i%20to%C3%A1n%202/NLP.ipynb#X41sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mT\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lequochuy/Documents/Research/UIT/b%C3%A0i%20to%C3%A1n%202/NLP.ipynb#X41sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m   text \u001b[39m=\u001b[39m lemmatize_passage(text)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lequochuy/Documents/Research/UIT/b%C3%A0i%20to%C3%A1n%202/NLP.ipynb#X41sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mreturn\u001b[39;00m text\n",
            "\u001b[1;32m/Users/lequochuy/Documents/Research/UIT/bài toán 2/NLP.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lequochuy/Documents/Research/UIT/b%C3%A0i%20to%C3%A1n%202/NLP.ipynb#X41sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlemmatize_passage\u001b[39m(text):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lequochuy/Documents/Research/UIT/b%C3%A0i%20to%C3%A1n%202/NLP.ipynb#X41sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     words \u001b[39m=\u001b[39m word_tokenize(text)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lequochuy/Documents/Research/UIT/b%C3%A0i%20to%C3%A1n%202/NLP.ipynb#X41sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     pos_tags \u001b[39m=\u001b[39m pos_tag(words)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lequochuy/Documents/Research/UIT/b%C3%A0i%20to%C3%A1n%202/NLP.ipynb#X41sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     lemmatizer \u001b[39m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lequochuy/Documents/Research/UIT/b%C3%A0i%20to%C3%A1n%202/NLP.ipynb#X41sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     lemmatized_words \u001b[39m=\u001b[39m [lemmatizer\u001b[39m.\u001b[39mlemmatize(word, get_wordnet_pos(tag)) \u001b[39mfor\u001b[39;00m word, tag \u001b[39min\u001b[39;00m pos_tags]\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/nltk/tag/__init__.py:168\u001b[0m, in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpos_tag\u001b[39m(tokens, tagset\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meng\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    144\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[39m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[39m    tag the given list of tokens.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m     tagger \u001b[39m=\u001b[39m _get_tagger(lang)\n\u001b[1;32m    169\u001b[0m     \u001b[39mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/nltk/tag/__init__.py:110\u001b[0m, in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    108\u001b[0m     tagger \u001b[39m=\u001b[39m PerceptronTagger(lang\u001b[39m=\u001b[39mlang)\n\u001b[1;32m    109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m     tagger \u001b[39m=\u001b[39m PerceptronTagger()\n\u001b[1;32m    111\u001b[0m \u001b[39mreturn\u001b[39;00m tagger\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/nltk/tag/perceptron.py:183\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[0;34m(self, load, lang)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m load:\n\u001b[0;32m--> 183\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_from_json(lang)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/nltk/tag/perceptron.py:273\u001b[0m, in \u001b[0;36mPerceptronTagger.load_from_json\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_from_json\u001b[39m(\u001b[39mself\u001b[39m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meng\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    272\u001b[0m     \u001b[39m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m     loc \u001b[39m=\u001b[39m find(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtaggers/averaged_perceptron_tagger_\u001b[39;49m\u001b[39m{\u001b[39;49;00mlang\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    274\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(loc \u001b[39m+\u001b[39m TAGGER_JSONS[lang][\u001b[39m\"\u001b[39m\u001b[39mweights\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39mas\u001b[39;00m fin:\n\u001b[1;32m    275\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mweights \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(fin)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/Users/lequochuy/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.8/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.8/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.8/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/lequochuy/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "#preprocessed_text = [text_sementic(text, 'S', False) for text in dic_text.values()]\n",
        "preprocessed_text = [text_sementic(text, 'T', False) for text in dic_text.values()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCFta3rReWCL",
        "outputId": "87b717f9-f295-4cda-8699-a901e3a5bf4a"
      },
      "outputs": [],
      "source": [
        "# Get Vectorize\n",
        "tfidf, data_vector = data_w_tfidf_vectorizer(preprocessed_text)\n",
        "print(tfidf.get_feature_names_out().shape, data_vector.shape)\n",
        "tfidf.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fsq2UPSuhnj"
      },
      "source": [
        "## Load answer Pre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UA1prhAbBXLm",
        "outputId": "59a26d4a-9644-497e-f81f-848842dd939a"
      },
      "outputs": [],
      "source": [
        "df_re, df_relabel = AddFeatureToAnswer(tfidf, path_prepost, \"PRE_LAB1/PreLab01.xlsx\", dic_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mws2hk3ns3kH",
        "outputId": "704a0b81-2c5f-4a49-d736-eeb5b896cef4"
      },
      "outputs": [],
      "source": [
        "print(df_re.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "EV2ek7rCo0ep",
        "outputId": "98bc795c-dfd6-4693-c575-9cdb6300444a"
      },
      "outputs": [],
      "source": [
        "\n",
        "n, bins = np.histogram(df_relabel)\n",
        "labels = [1, 2, 3, 4,5,6,7,8,9,10]\n",
        "bar_labels = ['red', 'blue', '_red', 'orange']\n",
        "bar_colors = ['blue', 'orange', 'green', 'red','purple','brown','pink','gray','olive','cyan']\n",
        "plt.bar(labels, n, label=labels, color=bar_colors)\n",
        "\n",
        "#plt.set_ylabel('fruit supply')\n",
        "#plt.set_title('Fruit supply by kind and color')\n",
        "plt.legend(title='Grade')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ1EREoGIHYz"
      },
      "source": [
        "## Load answer Post"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ydOEhLGyu2Pb",
        "outputId": "70593b21-ffd8-450e-e36e-727a1f3053aa"
      },
      "outputs": [],
      "source": [
        "df_res, df_reslabel = AddFeatureToAnswer(tfidf,path_prepost,\"POST_LAB1/PostLab01.xlsx\",dic_text)\n",
        "#WeightedEffort(df_res)\n",
        "print(df_res.shape,df_reslabel.shape)\n",
        "print(df_res.head())\n",
        "print(df_reslabel.info())\n",
        "df_reslabel.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok0aFWjFIZDU"
      },
      "source": [
        "## Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAB6Zn9dIgGN"
      },
      "source": [
        "## Naive Bayes model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__v-SDKtIjTF",
        "outputId": "beaac950-8c99-4336-86e8-de2dcd3a7bce"
      },
      "outputs": [],
      "source": [
        "#name_model = 'Naive Bayes'\n",
        "name_model = 'KNN'\n",
        "#name_model = 'Random Forests'\n",
        "model = ModelData(df_re, df_relabel, name_model,alpha = 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FCQg96Elfuka",
        "outputId": "8c0a0a85-8896-48c8-b800-27cc2810e495"
      },
      "outputs": [],
      "source": [
        "BestKNN(df_re, df_relabel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbrbSuJ9fyzp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qg69-TuLImbJ",
        "outputId": "8776941f-a0ee-4dfb-e749-6f2039a3b302"
      },
      "outputs": [],
      "source": [
        "#print(df_res.shape,df_reslabel.shape)\n",
        "predictions = EvaluateModel( model, df_res, df_reslabel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "p-RrX0WSIqfE",
        "outputId": "eaf8fea4-a9d5-4618-83ba-1d7b52102476"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(df_reslabel, predictions)\n",
        "\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm).plot();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQkLhZSLcijs"
      },
      "source": [
        "## Select features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXPh8HWfcnFc",
        "outputId": "f39f113c-424b-4eca-fc5e-0210d8a6572f"
      },
      "outputs": [],
      "source": [
        "ch2 = SelectKBest(chi2, k=22) #30\n",
        "ch2.fit_transform(df_re, df_relabel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-U4PTP_FgbG",
        "outputId": "829c1f57-6f22-4ce1-8a60-fac73e77ba8c"
      },
      "outputs": [],
      "source": [
        "#feature_names = ['mssv','count']\n",
        "print(df_re.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2O7YFdozctQz",
        "outputId": "b5af6fde-4191-43d5-9d4e-6fd3d764cb92"
      },
      "outputs": [],
      "source": [
        "#feature_names = ['mssv','count']\n",
        "#feature_names = feature_names.append(tfidf.get_feature_names_out())\n",
        "feature_names = df_re.columns\n",
        "#feature_names = ['mssv','count'].append(feature_names)\n",
        "feature_names_chi = [feature_names[i] for i in ch2.get_support(indices=True)]\n",
        "\n",
        "print(len(feature_names_chi))\n",
        "#feature_names_chi = feature_names_chi.append(['count'])\n",
        "print(feature_names_chi)\n",
        "feature_names_chi = feature_names_chi[1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jvaq4k1Rwm4H"
      },
      "source": [
        "## FEATURES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "lXajVsoG9khz",
        "outputId": "69c0e9f6-e4e3-48f1-e72c-400e7d758f48"
      },
      "outputs": [],
      "source": [
        "feature_names_chi = Words2Text(feature_names_chi)\n",
        "feature_names_chi = text_sementic(feature_names_chi, 'T', False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "KcHH8Iq9Fu_J",
        "outputId": "abf94873-0bde-46c0-9359-6294419bce6e"
      },
      "outputs": [],
      "source": [
        "feature_names_chi = Text2Words(feature_names_chi)\n",
        "feature_names_chi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-x5pzXVcya9"
      },
      "outputs": [],
      "source": [
        "tfidf, data_vector = data_w_tfidf_vectorizer(preprocessed_text,feature_names_chi)\n",
        "print(tfidf.get_feature_names_out().shape)\n",
        "tfidf.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dopwd7K-eB-t",
        "outputId": "99d9f8a0-c56a-4c86-9832-eefb009108be"
      },
      "outputs": [],
      "source": [
        "df_Prereduce, df_Prereducelabel = AddFeatureToAnswer(tfidf,path_prepost,\"PRE_LAB1/PreLab01.xlsx\",dic_text)\n",
        "#WeightedEffort(df_Prereduce)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0YLnkwReCX6",
        "outputId": "d9710b20-4c82-4083-9fa0-099607e46fcb"
      },
      "outputs": [],
      "source": [
        "df_resreduce, df_resreducelabel = AddFeatureToAnswer(tfidf,path_prepost,\"POST_LAB1/PostLab01.xlsx\",dic_text)\n",
        "#WeightedEffort(df_resreduce)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BC8NW5A3efpi",
        "outputId": "abf1dd12-d5a7-4b4d-ea3b-89e31d64049d"
      },
      "outputs": [],
      "source": [
        "print(df_resreduce.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7JyXaHAKhdyA",
        "outputId": "e02f7fd7-2854-437e-dc4e-7291e8b6e2e4"
      },
      "outputs": [],
      "source": [
        "BestKNN(df_Prereduce, df_Prereducelabel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OO8U8zxuesCj",
        "outputId": "a89f4792-a495-4f74-d2dd-847738828759"
      },
      "outputs": [],
      "source": [
        "model = ModelData(df_resreduce, df_resreducelabel, name_model,alpha = 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ObjVY6ves1N",
        "outputId": "17d7a333-dfc2-4582-de34-fe3a5ccd4b42"
      },
      "outputs": [],
      "source": [
        "predictions = EvaluateModel( model, df_resreduce, df_resreducelabel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "Uzd5hZ7uexsq",
        "outputId": "ba2775b0-8d95-4096-d4e7-97a6c418aad3"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(df_reslabel, predictions)\n",
        "\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm).plot();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_fiMH408JxD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
